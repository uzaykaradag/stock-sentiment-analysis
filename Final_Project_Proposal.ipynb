{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6ec0cf",
   "metadata": {},
   "source": [
    "# Stock Price Sentiment Analysis using Tweet Data\n",
    "\n",
    "## Team Members\n",
    "\n",
    "\n",
    "1. Uzay Karadağ | 090200738\n",
    "2. Hasan Çelik | 090180305\n",
    "3. Muhammed Fatih Kaya | 090200751\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We are going to use the **[tweepy](https://www.tweepy.org/)** module to gather tweets using the Twitter API for the given stock using queries containing the respective ticker.\n",
    "\n",
    "\n",
    "## Description of the Problem\n",
    "\n",
    "After gathering tweets about our respective stocks we will perform sentiment anlaysis on the tweets about the given ticker using various NLP modules. We will then use this sentiments to predict if the stock will close higher or lower than the opening price on that date. We will test the data using the price time series from the **yfinance** module.\n",
    "\n",
    "## Project Planning\n",
    "\n",
    "First we will start off by acquiring the raw tweets from the **tweepy** module and then process the raw text data using the several methods we learned during the class. We will then form a DataFrame using the **pandas** library where individual tickers and dates are listed properly.\n",
    "\n",
    "Second, we will do Exploratory Data Analysis to get a better picture on the dataset.\n",
    "\n",
    "We will then use **spaCy** and **NLTK** to perform sentiment analysis in the newly formed DataFrame. We may fine tune features and other parameters given the accuracy of our model.\n",
    "\n",
    "Lastly, we will test the model against historical price data gathered from **yfinance**. May circle back to Step 2 if needed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Project Pieces and Calendar\n",
    "\n",
    "1)\tData Acquisition | Uzay Karadağ ~ 1 week\n",
    "\n",
    "    a)\tGathering appropriate tweet dataset for related\n",
    "\n",
    "    b)\tGathering stock prices in the time interval suitable for the tweet dataset\n",
    "    \n",
    "2)\tData Cleaning and Preprocessing | Uzay Karadağ ~ 0.5 week\n",
    "\n",
    "    a)\tRemoving incorrect or irrelevant data from the dataset\n",
    "\n",
    "    b)\tMerge datasets at the appropriate time interval\n",
    "\n",
    "3)\tEDA | Hasan Çelik ~ 0.5 week\n",
    "\n",
    "    a)\tData visualization\n",
    "\n",
    "    b)\tDiscovering pattern\n",
    "\n",
    "4)\tAnalysis of EDA | Hasan Çelik ~ 1 week\n",
    "\n",
    "    a)\tSentiment Analysis of tweets with Natural Language Processing (NLP)\n",
    "\n",
    "5)\tLiterature Review, Feature Engineering | Fatih Kaya ~ 1 week\n",
    "\n",
    "    a)\tFeature Creation & Selection\n",
    "\n",
    "    b)\tReview of similar studies\n",
    "\n",
    "6)\tModel Construction | Fatih Kaya ~ 1 week\n",
    "\n",
    "    a)\tTrain - Test Splitting\n",
    "\n",
    "    b)\tPerforming several algorithms\n",
    "\n",
    "    c)\tModel Evaluation and choosing suitable modelling\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "### Hardware and Software\n",
    "\n",
    "1. Uzay Karadağ: 2017 MacBook Pro | 2,3 GHz Dual-Core Intel Core i5, 8GB RAM\n",
    "2. Fatih Kaya: Monster Abra A5 v15.5 | 2.4 GHz Quad-Core Intel Core i5, 16GB RAM \n",
    "3. Hasan Çelik: Dell Vostro 3515 |  2,6 GHz AMD Ryzen7, 16 GB RAM\n",
    "\n",
    "*Google Colab might be used for GPU and/or TPU computation if needed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8b749-6716-4e07-b63d-8907e480304b",
   "metadata": {},
   "source": [
    "## Atabey's notes\n",
    "\n",
    "I like the fact that you have a clear idea which tools you are going to use to collect the data. However, this is still a very weak proposal. Something written in less than an hour.\n",
    "\n",
    "You must show a sample of the data and explain each of the pieces. You must explain how you are going to clean the twitter data, also how you are going to match the timelines. The data you are going to collect from Yahoo is a time series, the same goes for the twitter data. You must also explain how you are going to convert text data (twitter data) into usable numerical time series data. \n",
    "\n",
    "As for the models: how you are going to check any correlations, cause/effect relationships between twitter data and yfinance data? What are your questions? What is your hypothesis? How are you going to test it? What ML algorithms are available? Which ones do you think are appropriate to use? I need more details.\n",
    "\n",
    "Also the labor division and time-table is not detailed enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2065d",
   "metadata": {},
   "source": [
    "### Some Notes\n",
    "Due to Twitter's guidelines of elevated access we failed to get access to the API to authorize tweepy. Currently working on it by contacting the related team at Twitter.\n",
    "Meanwhile we have found an alternative dataset which can be used in case Twitter rejects to give us elevated access rights once again.\n",
    "[The Alternative Dataset](https://www.kaggle.com/code/tommyupton/twitter-stock-market-sentiment-analysis/data?select=Tweet.csv). This dataset contains >3 million tweets about the tickers of 6 publicly traded companies which is plenty.\n",
    "\n",
    "After discussing we have decided upon doing binary classfication (0: closed lower than the opening price, 1: otw.) for each day depending on the sentiment scores of the day's tweets for the given companyt ticker. Our hypothesis is there is going to be enough correlation between the sentiment scores and the classification that we can accurately predict it. We will use one of the KNN or Logistic Regression / SVM as the ML algorithm.\n",
    "\n",
    "We will further explain how we are going to do the data cleaning and matching timelines part when we get a second return mail from Twitter using our personal Twitter Account (As they are older accounts maybe they will be granted access.)\n",
    "\n",
    "For sentiment analysis, we will use the Vader library. Following data preprocessing, we will use the polarity_scores() function of Vader to obtain each tweet's negative, positive, neutral, and compound scores. We can use the average emotion scores or generate weighted scores using columns such as likes to convert these scores into meaningful numbers for the model (more likes means more visibility).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}